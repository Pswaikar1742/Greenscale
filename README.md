# ğŸŒ± GreenScale

**Intelligent Scale-to-Zero Autoscaler for AI/ML Workloads on Kubernetes**

> Eliminate the cost of idle GPUs with event-driven autoscaling powered by KEDA.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.25+-326CE5.svg)](https://kubernetes.io)
[![KEDA](https://img.shields.io/badge/KEDA-2.12+-orange.svg)](https://keda.sh)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ¬ Product Demo

| Resource | Link |
|----------|------|
| ğŸ¥ **Demo Video** | [Watch Demo](https://drive.google.com/file/d/12IZwp1OVbmJUocjHKuo9kEB3Bkq5FRvj/view?usp=drive_link) |
| ğŸ“Š **Live Presentation** | [View Slides](https://drive.google.com/file/d/1iT0e_xwR-3lJLyux35Hz4fF4ixFNuKZ2/view?usp=sharing) |
| ğŸŒ **Hosted App** | *Run locally with one command (see below)* |

> **Quick Demo:** Run `./scripts/run-greenscale.sh` and open http://localhost:8501

---

## ğŸ¯ Problem Statement

### The $2.7 Billion Problem

Organizations running AI/ML workloads on Kubernetes face a critical cost challenge:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE IDLE GPU PROBLEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ğŸ’° A100 GPU Cost: $3.50/hour = $2,520/month                   â”‚
â”‚   ğŸ“Š Average AI Workload Utilization: Only 5-15%                â”‚
â”‚   ğŸ”¥ Wasted Cost: Up to $2,394/month PER GPU                    â”‚
â”‚                                                                 â”‚
â”‚   "GPUs sit idle 85-95% of the time, but you pay 100%"         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why does this happen?**
- Traditional Kubernetes keeps minimum replicas running 24/7
- Batch AI jobs are sporadic (inference requests, model training)
- No native "scale to zero" for GPU workloads
- Manual scaling is error-prone and slow

---

## ğŸ’¡ Our Solution: GreenScale

GreenScale is an **event-driven autoscaling platform** that enables true **Scale-to-Zero** for AI/ML workloads:

| Feature | Traditional K8s | GreenScale |
|---------|-----------------|------------|
| Minimum Replicas | 1+ (always on) | **0** (truly off) |
| GPU Cost at Idle | $2,520/month | **$0/month** |
| Scale-up Time | Manual / HPA lag | **~2 seconds** |
| Scale Trigger | CPU/Memory metrics | **Event-driven (queue)** |

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   No Jobs    â”‚     â”‚  Job Arrives â”‚     â”‚  Processing  â”‚
â”‚              â”‚     â”‚              â”‚     â”‚              â”‚
â”‚  Workers: 0  â”‚â”€â”€â”€â”€â–¶â”‚  Workers: 1  â”‚â”€â”€â”€â”€â–¶â”‚  Workers: N  â”‚
â”‚  Cost: $0    â”‚     â”‚  (2s cold)   â”‚     â”‚  (auto-scale)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
                     â”‚  Job Done    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚              â”‚   30s cooldown
                     â”‚  Workers: 0  â”‚
                     â”‚  Cost: $0    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real-World Savings Calculator

| Scenario | Traditional | GreenScale | Monthly Savings |
|----------|-------------|------------|-----------------|
| Dev/Test (5% util) | $2,520 | $126 | **$2,394** |
| Staging (15% util) | $2,520 | $378 | **$2,142** |
| Production (30% util) | $2,520 | $756 | **$1,764** |

---

## ğŸ› ï¸ Technology Stack

### Core Technologies

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Orchestration** | Kubernetes | Container orchestration |
| **Autoscaling** | KEDA | Event-driven scale-to-zero |
| **Message Queue** | Redis | Job queue & result storage |
| **AI Backend** | Llama 3.3 70B (Neysa) | LLM inference API |
| **Frontend** | Streamlit | Real-time dashboard |
| **Containerization** | Docker | Worker containerization |

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER REQUEST                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT DASHBOARD                          â”‚
â”‚                         (src/app.py)                             â”‚
â”‚              â€¢ Submit prompts  â€¢ View results                    â”‚
â”‚              â€¢ Real-time metrics  â€¢ Cost tracking                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          REDIS                                   â”‚
â”‚                    Message Broker                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚ jobs (list) â”‚        â”‚ result:{id} (kv)â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         KEDA            â”‚      â”‚      WORKER PODS                â”‚
â”‚   Event-Driven Scaler   â”‚â”€â”€â”€â”€â”€â–¶â”‚     (src/worker.py)             â”‚
â”‚  â€¢ Monitors Redis queue â”‚      â”‚  â€¢ Replicas: 0 â†’ 5              â”‚
â”‚  â€¢ 30s cooldown         â”‚      â”‚  â€¢ Calls Llama 3.3 70B API      â”‚
â”‚  â€¢ Instant scale-up     â”‚      â”‚  â€¢ Stores results in Redis      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | File | Description |
|-----------|------|-------------|
| Dashboard | `src/app.py` | Streamlit UI with real-time metrics |
| Worker | `src/worker.py` | Processes jobs from Redis queue |
| KEDA Config | `k8s/keda-scaledobject.yaml` | Scale-to-zero configuration |
| Redis | `k8s/redis.yaml` | Message queue deployment |

---

## ğŸ“ Project Structure

```
greenscale/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # Streamlit frontend dashboard
â”‚   â””â”€â”€ worker.py           # K8s worker - processes AI jobs
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ namespace.yaml      # greenscale-system namespace
â”‚   â”œâ”€â”€ redis.yaml          # Redis deployment + service
â”‚   â”œâ”€â”€ worker-deployment.yaml  # Worker deployment (replicas: 0)
â”‚   â”œâ”€â”€ keda-scaledobject.yaml  # KEDA autoscaling config
â”‚   â””â”€â”€ openai-secret.yaml  # API key secret
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run-greenscale.sh   # â­ One-click deployment script
â”‚   â””â”€â”€ test-queue.sh       # E2E test script
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md # Comprehensive deployment guide
â”‚   â”œâ”€â”€ UI_METRICS_GUIDE.md # Dashboard metrics explanation
â”‚   â””â”€â”€ ...                 # Additional documentation
â”œâ”€â”€ Dockerfile              # Worker container image
â”œâ”€â”€ docker-compose.yaml     # Local development setup
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Quick Start (One Command)

### Prerequisites

- **Docker** (v20.10+)
- **Minikube** (v1.30+)
- **kubectl** (v1.27+)
- **Python** (3.9+)

### One-Click Deployment

```bash
# Clone the repository
git clone https://github.com/Pswaikar1742/Greenscale.git
cd Greenscale

# Run everything with one command!
./scripts/run-greenscale.sh
```

This script automatically:
1. âœ… Starts Minikube cluster
2. âœ… Installs KEDA autoscaler
3. âœ… Builds Docker image
4. âœ… Deploys all Kubernetes resources
5. âœ… Sets up Redis port-forwarding
6. âœ… Launches Streamlit dashboard

**Open http://localhost:8501** and start submitting AI jobs! ğŸ‰

> ğŸ“š For detailed setup options, see [docs/DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)

---

## ğŸ§ª See Scale-to-Zero in Action

### Watch the magic happen:

```bash
# Terminal 1: Watch pods (should show 0 worker pods initially)
kubectl get pods -n greenscale-system -w

# Terminal 2: Submit a job via dashboard or CLI
kubectl exec -n greenscale-system deployment/redis -- \
  redis-cli LPUSH jobs '{"job_id":"test-001","prompt":"What is 2+2?"}'

# Watch Terminal 1: Worker scales 0â†’1, processes job, then 1â†’0 after 30s
```

**Expected behavior:**
```
NAME                                 READY   STATUS    
redis-xxxxxxxxxx-xxxxx               1/1     Running   
greenscale-worker-xxxxxxxxxx-xxxxx   0/1     Pending   â† Job arrives
greenscale-worker-xxxxxxxxxx-xxxxx   1/1     Running   â† Processing
greenscale-worker-xxxxxxxxxx-xxxxx   0/1     Terminating â† 30s cooldown
(no worker pods)                                       â† Scale-to-Zero!
```

---

## âš™ï¸ Configuration

### KEDA ScaledObject

| Parameter | Value | Description |
|-----------|-------|-------------|
| `minReplicaCount` | 0 | Enable Scale-to-Zero |
| `maxReplicaCount` | 5 | Max parallel workers |
| `cooldownPeriod` | 30 | Seconds before scale-down |
| `pollingInterval` | 5 | Queue check frequency |
| `listLength` | 1 | Scale up when â‰¥1 job |

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `NEYSA_API_KEY` | Llama API authentication | Required |
| `NEYSA_API_URL` | AI endpoint URL | `https://boomai-llama.neysa.io/v1/chat/completions` |
| `REDIS_HOST` | Redis hostname | `redis-service` |
| `REDIS_PORT` | Redis port | `6379` |

---

## ğŸ“Š Dashboard Features

The Streamlit dashboard provides real-time visibility:

| Metric | Description |
|--------|-------------|
| ğŸ“¥ **Queue** | Jobs waiting in Redis |
| âš¡ **Workers** | Active worker pods (0-5) |
| âœ… **Processed** | Total completed jobs |
| ğŸ’° **Savings** | Estimated cost savings |

> ğŸ“š For detailed metrics explanation, see [docs/UI_METRICS_GUIDE.md](docs/UI_METRICS_GUIDE.md)

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Worker not scaling up | Check KEDA: `kubectl get scaledobject -n greenscale-system` |
| Redis connection failed | Ensure port-forward is running |
| API errors | Verify API key in secret |

> ğŸ“š For more issues, see [docs/DEPLOYMENT_GUIDE.md#troubleshooting](docs/DEPLOYMENT_GUIDE.md#troubleshooting)

---

## ğŸ‘¥ Team

| Member | Role | Responsibilities |
|--------|------|------------------|
| **Prathmesh (P)** | Platform Engineer | Kubernetes, Docker, KEDA, Infrastructure |
| **Ali (A)** | Application Engineer | Python, Redis, Streamlit UI |

---

## ğŸ“œ License

MIT License - Built for **AIBoomi Hackathon 2026**

---

## ğŸ”— Links

| Resource | URL |
|----------|-----|
| ğŸ“‚ GitHub Repo | [github.com/Pswaikar1742/Greenscale](https://github.com/Pswaikar1742/Greenscale.git) |
| ğŸ¥ Demo Video | [Watch Demo](https://drive.google.com/file/d/12IZwp1OVbmJUocjHKuo9kEB3Bkq5FRvj/view?usp=drive_link) |
| ğŸ“Š Presentation | [View Slides](https://drive.google.com/file/d/1iT0e_xwR-3lJLyux35Hz4fF4ixFNuKZ2/view?usp=sharing) |

---

<p align="center">
  <b>ğŸŒ± GreenScale - Because idle GPUs shouldn't cost you money</b>
  <br><br>
  <i>Built with â¤ï¸ for AIBoomi Hackathon 2026</i>
</p>
