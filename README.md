# ğŸŒ± GreenScale

**Intelligent Scale-to-Zero Autoscaler for AI/ML Workloads on Kubernetes**

> Eliminate the cost of idle GPUs with event-driven autoscaling powered by KEDA.

## ğŸ¯ What is GreenScale?

GreenScale is a DevTool/Infrastructure project that enables **Scale-to-Zero** for AI/ML workloads. When there's no work to do, your expensive GPU pods sleep. When jobs arrive, they wake up instantly.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚â”€â”€â”€â”€â–¶â”‚    Redis    â”‚â—€â”€â”€â”€â”€â”‚      KEDA        â”‚
â”‚    (app.py)     â”‚     â”‚   (jobs)    â”‚     â”‚  (ScaledObject)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                     â”‚
                               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼    â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Worker Pod(s)   â”‚
                        â”‚   (worker.py)    â”‚
                        â”‚  replicas: 0â†’N   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Orchestrator | Kubernetes (Minikube) |
| Autoscaler | KEDA |
| Message Broker | Redis |
| Language | Python 3.9+ |
| Frontend | Streamlit |
| Containerization | Docker |
| AI Engine | OpenAI API |

## ğŸ“ Project Structure

```
greenscale/
â”œâ”€â”€ k8s/                          # Kubernetes manifests (P)
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ redis-deployment.yaml
â”‚   â”œâ”€â”€ redis-service.yaml
â”‚   â”œâ”€â”€ worker-deployment.yaml
â”‚   â”œâ”€â”€ scaledobject.yaml
â”‚   â””â”€â”€ secrets.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                    # Streamlit frontend (A)
â”‚   â””â”€â”€ worker.py                 # Job processor (A)
â”œâ”€â”€ Dockerfile                    # Worker container (P)
â”œâ”€â”€ docker-compose.yaml           # Local dev environment
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites
- Docker Desktop
- Minikube
- kubectl
- KEDA installed on cluster

### 1. Start Minikube
```bash
minikube start --driver=docker
```

### 2. Install KEDA
```bash
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
helm install keda kedacore/keda --namespace keda --create-namespace
```

### 3. Deploy GreenScale
```bash
# Create namespace
kubectl apply -f k8s/namespace.yaml

# Deploy Redis
kubectl apply -f k8s/redis-deployment.yaml
kubectl apply -f k8s/redis-service.yaml

# Deploy secrets (update with your API key first!)
kubectl apply -f k8s/secrets.yaml

# Deploy worker (starts at 0 replicas)
kubectl apply -f k8s/worker-deployment.yaml

# Enable KEDA autoscaling
kubectl apply -f k8s/scaledobject.yaml
```

### 4. Run the Frontend (locally)
```bash
pip install -r requirements.txt
streamlit run src/app.py
```

## ğŸ‘¥ Team

- **P (Prathmesh)** - Platform Engineer: K8s, Docker, Infrastructure
- **A (Ali)** - Application Engineer: Python, Redis, Streamlit UI

## ğŸ“œ License

MIT License - Built for AIBoomi Hackathon 2026 